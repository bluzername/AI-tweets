#!/usr/bin/env python3
"""Full pipeline test with mocked transcription to test all components."""

import logging
import os
import json
from datetime import datetime
from pathlib import Path

# Set up logging
logging.basicConfig(level=logging.INFO)

def create_test_setup():
    """Create test config and mock transcription."""
    # Create test config
    test_config = {
        "podcasts": [
            {
                "name": "Test Podcast",
                "rss_url": "https://www.thisamericanlife.org/podcast/rss.xml",
                "x_handle": "ThisAmerLife",
                "creator_handles": ["iraglass"]
            }
        ],
        "accounts": [
            {
                "name": "main",
                "target_audience": "Test audience interested in great storytelling",
                "tone": "conversational",
                "emoji_usage": "minimal"
            }
        ]
    }
    
    with open('test_config.json', 'w') as f:
        json.dump(test_config, f, indent=2)
    
    # Create test .env without real keys
    test_env = """# Test Configuration
OPENAI_API_KEY=test_key_for_demo
WHISPER_MODEL=whisper-1
GPT_MODEL=gpt-4o-mini
TRANSCRIPTION_METHODS=youtube,whisper
DAYS_BACK=7
MAX_HIGHLIGHTS=3
DRY_RUN=true
LOG_LEVEL=INFO
"""
    
    with open('test.env', 'w') as f:
        f.write(test_env)
    
    print("‚úÖ Created test configuration files")

def test_rss_and_youtube_detection():
    """Test RSS parsing and YouTube URL detection."""
    print("\n=== Testing RSS + YouTube URL Detection ===")
    
    from src.rss_parser import RSSFeedParser
    
    parser = RSSFeedParser(
        feed_urls=["https://www.thisamericanlife.org/podcast/rss.xml"],
        days_back=7
    )
    
    episodes = parser.parse_feeds()
    if episodes:
        print(f"‚úÖ Found {len(episodes)} recent episodes")
        
        for i, ep in enumerate(episodes[:3]):
            print(f"  {i+1}. {ep.title}")
            print(f"     Audio: {ep.audio_url[:50]}...")
            print(f"     YouTube: {ep.youtube_urls or 'None'}")
            print(f"     Duration: {ep.duration or 'Unknown'} seconds")
        
        return episodes[0]
    else:
        print("‚ùå No episodes found")
        return None

def test_multi_transcriber_with_mock():
    """Test multi-transcriber with mock data."""
    print("\n=== Testing MultiTranscriber with Mock Data ===")
    
    from src.multi_transcriber import MultiTranscriber
    
    # Test initialization
    transcriber = MultiTranscriber(
        openai_api_key="test_key",
        preferred_methods=["youtube", "whisper"]
    )
    
    stats = transcriber.get_transcription_stats()
    print(f"‚úÖ MultiTranscriber configured:")
    print(f"   Methods available: {stats['methods_available']}")
    print(f"   Preferred order: {stats['preferred_order']}")
    
    # Mock a transcription result (what it would look like if APIs worked)
    mock_result = {
        "text": "This is a sample transcript from a podcast episode about an interesting topic. The host discusses various fascinating points and interviews an expert guest who shares valuable insights about the subject matter.",
        "language": "en",
        "duration": 1800,  # 30 minutes
        "method_used": "youtube",
        "source": "youtube"
    }
    
    print(f"‚úÖ Mock transcription result: {len(mock_result['text'])} characters")
    return mock_result

def test_ai_analyzer_structure():
    """Test AI analyzer structure without real API calls."""
    print("\n=== Testing AI Analyzer Structure ===")
    
    from src.ai_analyzer import AIAnalyzer
    
    try:
        analyzer = AIAnalyzer(api_key="test_key", model="gpt-4o-mini")
        print("‚úÖ AI Analyzer initialized")
        
        # Mock what highlights would look like
        mock_highlights = [
            {
                "content": "The key insight about human psychology is that we often underestimate our ability to adapt to change.",
                "timestamp": "12:30",
                "score": 0.95,
                "category": "psychology"
            },
            {
                "content": "Three actionable steps for building better habits: start small, be consistent, and track progress.",
                "timestamp": "18:45", 
                "score": 0.87,
                "category": "self-improvement"
            },
            {
                "content": "The surprising research finding that challenges conventional wisdom about productivity.",
                "timestamp": "25:15",
                "score": 0.82,
                "category": "research"
            }
        ]
        
        print(f"‚úÖ Mock highlights generated: {len(mock_highlights)} insights")
        return mock_highlights
        
    except Exception as e:
        print(f"‚ùå AI Analyzer failed: {e}")
        return []

def test_thread_generator_structure():
    """Test thread generator structure."""
    print("\n=== Testing Thread Generator Structure ===")
    
    from src.thread_generator import ThreadGenerator, ThreadStyle
    
    try:
        generator = ThreadGenerator(api_key="test_key", model="gpt-4o-mini")
        
        # Mock thread style
        style = ThreadStyle(
            tone="conversational",
            emoji_usage="minimal", 
            hashtag_style="trending",
            cta_style="medium"
        )
        
        print("‚úÖ Thread Generator initialized")
        print(f"   Style: {style.tone}, emojis: {style.emoji_usage}")
        
        # Mock what a generated thread would look like
        mock_thread = [
            "üß† Just listened to an incredible podcast about human psychology and habit formation. Here are 3 mind-blowing insights: üßµ (1/4)",
            
            "1/ We massively underestimate our ability to adapt to change. The host shared research showing people bounce back from major life events much faster than predicted. This explains why we often fear change more than we should.",
            
            "2/ Building better habits comes down to 3 simple steps:\n‚Ä¢ Start ridiculously small (2 min rule)\n‚Ä¢ Be consistent over perfect\n‚Ä¢ Track progress visually\n\nThe guest's framework has helped thousands stick to new routines.",
            
            "3/ The most surprising finding: productivity 'hacks' often backfire. The research shows that sustainable progress comes from boring consistency, not clever shortcuts.\n\nSometimes the simplest approach is the most powerful. üí°\n\nThoughts? üëá"
        ]
        
        print(f"‚úÖ Mock thread generated: {len(mock_thread)} tweets")
        for i, tweet in enumerate(mock_thread):
            print(f"   Tweet {i+1}: {len(tweet)} chars - {tweet[:50]}...")
        
        return mock_thread
        
    except Exception as e:
        print(f"‚ùå Thread Generator failed: {e}")
        return []

def test_x_publisher_structure():
    """Test X publisher structure."""
    print("\n=== Testing X Publisher Structure ===") 
    
    from src.x_publisher import MultiAccountPublisher
    
    try:
        # Mock credentials (won't actually work but tests structure)
        mock_accounts = {
            "main": {
                "api_key": "test",
                "api_secret": "test", 
                "access_token": "test",
                "access_token_secret": "test",
                "bearer_token": "test"
            }
        }
        
        publisher = MultiAccountPublisher(mock_accounts)
        print("‚úÖ X Publisher initialized with mock credentials")
        
        return True
        
    except Exception as e:
        print(f"‚ùå X Publisher failed: {e}")
        return False

def test_config_loading():
    """Test config with test files."""
    print("\n=== Testing Configuration Loading ===")
    
    from src.config import Config
    
    try:
        config = Config(config_file="test_config.json", env_file="test.env")
        
        print(f"‚úÖ Config loaded successfully:")
        print(f"   Podcasts: {len(config.podcasts)}")
        print(f"   Accounts: {len(config.accounts)}")
        print(f"   Transcription methods: {config.transcription_methods}")
        print(f"   Max highlights: {config.max_highlights}")
        print(f"   Dry run: {config.dry_run}")
        
        return config
        
    except Exception as e:
        print(f"‚ùå Config loading failed: {e}")
        return None

def simulate_full_pipeline():
    """Simulate the full pipeline flow."""
    print("\n=== Simulating Full Pipeline ===")
    
    # Test each component in sequence
    episode = test_rss_and_youtube_detection()
    if not episode:
        return False
    
    transcription = test_multi_transcriber_with_mock()
    if not transcription:
        return False
    
    highlights = test_ai_analyzer_structure()
    if not highlights:
        return False
    
    thread = test_thread_generator_structure()
    if not thread:
        return False
    
    if not test_x_publisher_structure():
        return False
    
    config = test_config_loading()
    if not config:
        return False
    
    print("\n‚úÖ Full pipeline simulation successful!")
    print("\nWhat would happen in production:")
    print(f"1. üì° Fetch {episode.title}")
    print(f"2. üéôÔ∏è  Transcribe via {transcription['method_used']} ({transcription['duration']}s)")
    print(f"3. üß† Extract {len(highlights)} key insights")  
    print(f"4. üî• Generate {len(thread)}-tweet thread")
    print(f"5. üì± Post to X.com or save to markdown")
    
    return True

def cleanup():
    """Remove test files."""
    for file in ['test_config.json', 'test.env']:
        if os.path.exists(file):
            os.remove(file)
    print("üßπ Cleaned up test files")

def main():
    """Run comprehensive pipeline test."""
    print("AI-tweets Pipeline Integration Test")
    print("=" * 60)
    
    try:
        create_test_setup()
        success = simulate_full_pipeline()
        
        print("\n" + "=" * 60)
        if success:
            print("üéâ PIPELINE TEST PASSED!")
            print("\nThe AI-tweets pipeline is structurally sound and ready for use.")
            print("\nTo run with real data:")
            print("1. Add valid OpenAI API key to .env")
            print("2. For local testing: TRANSCRIPTION_METHODS=whisper (avoid YouTube blocking)")
            print("3. For production: TRANSCRIPTION_METHODS=youtube,whisper (cost optimized)")
            print("4. Run: python main.py --dry-run --limit 1")
        else:
            print("‚ùå PIPELINE TEST FAILED!")
            print("Check the errors above for issues to fix.")
            
    finally:
        cleanup()
    
    return success

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)